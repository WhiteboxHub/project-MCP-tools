{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents Basics with MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super simple version first\n",
      "\n",
      "- What MCP is: Think of it like USB‑C for AI. It’s a standard “plug” that lets an AI app (your agent) safely talk to outside tools and data without bespoke wiring each time. You write small MCP servers that expose capabilities, and any MCP‑aware agent can discover and use them. ([modelcontextprotocol.info](https://modelcontextprotocol.info/en/docs/introduction/?utm_source=chatgpt.com))\n",
      "- The pieces:\n",
      "  - MCP server: a tiny program that offers tools (functions), resources (read‑only data), and optional prompts to the model. ([modelcontextprotocol.io](https://modelcontextprotocol.io/quickstart?utm_source=chatgpt.com))\n",
      "  - MCP host/client (your agent or an app like Claude Desktop): connects to one or more servers, lists their tools, and lets the model call them with your approval. ([modelcontextprotocol.info](https://modelcontextprotocol.info/en/docs/introduction/?utm_source=chatgpt.com))\n",
      "  - Transports: clients and servers talk over simple channels like stdio or HTTP; you don’t have to care about the wire format. ([github.com](https://github.com/modelcontextprotocol/python-sdk?utm_source=chatgpt.com))\n",
      "- One safety rule: only connect to servers you trust, especially ones that touch the internet, because tool calls can be abused (prompt injection, data exfiltration). ([docs.anthropic.com](https://docs.anthropic.com/en/docs/claude-code/mcp?utm_source=chatgpt.com))\n",
      "\n",
      "Build a tiny agent in 5 minutes\n",
      "\n",
      "We’ll do two things: (1) make a toy MCP server that adds two numbers; (2) run it and try it from a client.\n",
      "\n",
      "1) Make a 10‑line MCP server (Python)\n",
      "\n",
      "- Install the SDK/CLI: pip install \"mcp[cli]\"\n",
      "- Create server.py with this code:\n",
      "\n",
      "```\n",
      "from mcp.server.fastmcp import FastMCP\n",
      "\n",
      "mcp = FastMCP(\"Demo\")\n",
      "\n",
      "@mcp.tool()\n",
      "def add(a: int, b: int) -> int:\n",
      "    \"Add two numbers\"\n",
      "    return a + b\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    mcp.run(transport=\"stdio\")\n",
      "```\n",
      "\n",
      "- Test it locally with the built‑in inspector:\n",
      "  - mcp dev server.py\n",
      "  - Click the tool, pass {\"a\":2,\"b\":3}, see 5 come back.\n",
      "These steps use the official Python SDK’s FastMCP and dev tools. ([github.com](https://github.com/modelcontextprotocol/python-sdk?utm_source=chatgpt.com))\n",
      "\n",
      "2) Let an AI use it (two easy options)\n",
      "\n",
      "- Easiest: plug into an existing host.\n",
      "  - Claude Desktop can load your server so the model can call add when needed.\n",
      "  - Command: mcp install server.py, then enable it inside Claude Desktop. ([github.com](https://github.com/modelcontextprotocol/python-sdk?utm_source=chatgpt.com), [modelcontextprotocol.io](https://modelcontextprotocol.io/quickstart?utm_source=chatgpt.com))\n",
      "\n",
      "- Roll your own ultra‑simple client (Node)\n",
      "  - There’s a short tutorial that shows a tiny MCP client: it spawns your server, lists tools, and runs an interactive chat loop where the model can call those tools. Follow its steps (npm init, install @modelcontextprotocol/sdk, etc.). ([modelcontextprotocol.io](https://modelcontextprotocol.io/tutorials/building-a-client-node?utm_source=chatgpt.com))\n",
      "\n",
      "What’s actually happening (in plain English)\n",
      "\n",
      "- Your agent connects to the server and asks “what can you do?” The server replies with a list like: tool name add, description “Add two numbers,” and the JSON schema for inputs.\n",
      "- When the model decides it needs that tool, the agent calls it via MCP and hands the result back to the model, which finishes the answer. All of that is standardized by MCP, so you can swap servers or models without rewiring. ([modelcontextprotocol.io](https://modelcontextprotocol.io/quickstart?utm_source=chatgpt.com), [github.com](https://github.com/modelcontextprotocol/python-sdk?utm_source=chatgpt.com))\n",
      "\n",
      "Where to go next\n",
      "\n",
      "- Expand your server with more tools (files, APIs) and resources; the official quickstarts walk through a weather server and other examples. ([modelcontextprotocol.io](https://modelcontextprotocol.io/quickstart?utm_source=chatgpt.com))\n",
      "- If you prefer Python end‑to‑end, you can also build a custom client with the Python SDK and connect over stdio or HTTP; the same basic flow applies. ([github.com](https://github.com/modelcontextprotocol/python-sdk?utm_source=chatgpt.com))\n",
      "\n",
      "If you tell me your preferred language (Python or Node) and model provider, I can paste a minimal “agent loop” that wires MCP tool discovery into a single chat endpoint.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, WebSearchTool\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    model=\"gpt-5\",\n",
    "    tools=[\n",
    "        WebSearchTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "result = await Runner.run(agent, \"What is the model context protocol and how to build a simple agent with it in super simpler terms?\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch_weather\n",
      "Fetch the weather for a given location.\n",
      "{\n",
      "  \"$defs\": {\n",
      "    \"Location\": {\n",
      "      \"properties\": {\n",
      "        \"lat\": {\n",
      "          \"title\": \"Lat\",\n",
      "          \"type\": \"number\"\n",
      "        },\n",
      "        \"long\": {\n",
      "          \"title\": \"Long\",\n",
      "          \"type\": \"number\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"lat\",\n",
      "        \"long\"\n",
      "      ],\n",
      "      \"title\": \"Location\",\n",
      "      \"type\": \"object\",\n",
      "      \"additionalProperties\": false\n",
      "    }\n",
      "  },\n",
      "  \"properties\": {\n",
      "    \"location\": {\n",
      "      \"description\": \"The location to fetch the weather for.\",\n",
      "      \"properties\": {\n",
      "        \"lat\": {\n",
      "          \"title\": \"Lat\",\n",
      "          \"type\": \"number\"\n",
      "        },\n",
      "        \"long\": {\n",
      "          \"title\": \"Long\",\n",
      "          \"type\": \"number\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"lat\",\n",
      "        \"long\"\n",
      "      ],\n",
      "      \"title\": \"Location\",\n",
      "      \"type\": \"object\",\n",
      "      \"additionalProperties\": false\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"location\"\n",
      "  ],\n",
      "  \"title\": \"fetch_weather_args\",\n",
      "  \"type\": \"object\",\n",
      "  \"additionalProperties\": false\n",
      "}\n",
      "\n",
      "fetch_data\n",
      "Read the contents of a file.\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"path\": {\n",
      "      \"description\": \"The path to the file to read.\",\n",
      "      \"title\": \"Path\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"directory\": {\n",
      "      \"anyOf\": [\n",
      "        {\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "          \"type\": \"null\"\n",
      "        }\n",
      "      ],\n",
      "      \"description\": \"The directory to read the file from.\",\n",
      "      \"title\": \"Directory\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"path\",\n",
      "    \"directory\"\n",
      "  ],\n",
      "  \"title\": \"fetch_data_args\",\n",
      "  \"type\": \"object\",\n",
      "  \"additionalProperties\": false\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from typing_extensions import TypedDict, Any\n",
    "\n",
    "from agents import Agent, FunctionTool, RunContextWrapper, function_tool\n",
    "\n",
    "\n",
    "class Location(TypedDict):\n",
    "    lat: float\n",
    "    long: float\n",
    "\n",
    "@function_tool  \n",
    "async def fetch_weather(location: Location) -> str:\n",
    "    \n",
    "    \"\"\"Fetch the weather for a given location.\n",
    "\n",
    "    Args:\n",
    "        location: The location to fetch the weather for.\n",
    "    \"\"\"\n",
    "    # In real life, we'd fetch the weather from a weather API\n",
    "    return f\"The weather in this location: {location.lat}, {location.long} is sunny\"\n",
    "\n",
    "\n",
    "@function_tool(name_override=\"fetch_data\")  \n",
    "def read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:\n",
    "    \"\"\"Read the contents of a file.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the file to read.\n",
    "        directory: The directory to read the file from.\n",
    "    \"\"\"\n",
    "    # In real life, we'd read the file from the file system\n",
    "    return \"<file contents>\"\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    tools=[fetch_weather, read_file],  \n",
    ")\n",
    "\n",
    "for tool in agent.tools:\n",
    "    if isinstance(tool, FunctionTool):\n",
    "        print(tool.name)\n",
    "        print(tool.description)\n",
    "        print(json.dumps(tool.params_json_schema, indent=2))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
